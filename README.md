# Deep Learning Journey

## 📑 Table of Contents

- [📘 About](#-about)
- [📚 Course Content](#-course-content)
- [🧠 What I've Learned](#-what-ive-learned)
- [📖 Supplementary Reading](#-supplementary-reading)
- [🎓 Certificate](#-certificate)

## 📘 About

This repository documents my self-study of Deep Learning through an online Udemy course: **[The Complete Neural Networks Bootcamp: Theory, Applications](https://www.udemy.com/course/the-complete-neural-networks-bootcamp-theory-applications/)** covering core topics like MLP, CNN, RNN, Transformers, and modern applications such as NLP, CV, and LLMs.

## 🧠 What I've Learned

## 📖 Supplementary Reading

## 🎓 Certificate

## 📚 Course content

| Section    | Title            | Summary                                                                                                       | Note and Code                |
|------------|------------------|---------------------------------------------------------------------------------------------------------------|------------------------------|
| 1  | How Neural Networks and Backpropagation Works     | - Theory of Gradient Descent & Propagation<br>- Fwd/Bwd  calculating for a MLP | ✍️ [Write](https://github.com/laikhanhhoang/Deep_Learning_Journey/blob/main/Lecture_Note/Section%201%20-%20How%20Neural%20Networks%20and%20Back%20Propagation%20Work.pdf) <br>💻 No |
|2| Loss Functions| - MSE, MAE, Huber Loss, Binary Cross Entropy, Softmax Function, KL Divergence, Hinge Loss, Triplet Ranking Loss <br> - Pros, cons, and real-world applications of the above loss functions| ✍️ [Write](https://github.com/laikhanhhoang/Deep_Learning_Journey/blob/main/Lecture_Note/Section%202%20-%20Loss%20Functions.pdf) <br>💻 No |
|3| Activation Function| - Sigmoid, Tanh, ReLU, PreLU, ELU, GLU, Swish, Mish <br> - Properties of the above activation functions and their relation to the vanishing gradient problem| ✍️ [Write](https://github.com/laikhanhhoang/Deep_Learning_Journey/blob/main/Lecture_Note/Section%203%20-%20Activation%20Functions.pdf) <br>💻 No |
|4| Regularization and Normalization| - Regularization techniques: L1, L2, Dropout, DropConnect <br> - Normalization techniques: Min-Max, Standardization, Batch Normalization, Layer Normalization, Group Normalization <br> - Pseudo-code for those layers| ✍️ [Write](https://github.com/laikhanhhoang/Deep_Learning_Journey/blob/main/Lecture_Note/Section%204%20-%20Regularization%20and%20Normalization.pdf) <br>💻 No |
