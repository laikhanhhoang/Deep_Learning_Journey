# Deep Learning Journey

## ğŸ“‘ Table of Contents

- [ğŸ“˜ About](#-about)
- [ğŸ“š Course Content](#-course-content)
- [ğŸ§  What I've Learned](#-what-ive-learned)
- [ğŸ“– Supplementary Reading](#-supplementary-reading)
- [ğŸ“ Certificate](#-certificate)

## ğŸ“˜ About

This repository documents my self-study of Deep Learning through an online Udemy course: **[The Complete Neural Networks Bootcamp: Theory, Applications](https://www.udemy.com/course/the-complete-neural-networks-bootcamp-theory-applications/)** covering core topics like MLP, CNN, RNN, Transformers, and modern applications such as NLP, CV, and LLMs.

## ğŸ§  What I've Learned

## ğŸ“– Supplementary Reading

## ğŸ“ Certificate

## ğŸ“š Course content

| Section    | Title            | Summary                                                                                                       | Note and Code                |
|------------|------------------|---------------------------------------------------------------------------------------------------------------|------------------------------|
| 1  | How Neural Networks and Backpropagation Works     | - Theory of Gradient Descent & Propagation<br>- Fwd/Bwd  calculating for a MLP | âœï¸ [Written](https://github.com/laikhanhhoang/Deep_Learning_Journey/blob/main/Lecture_Note/Section%201%20-%20How%20Neural%20Networks%20and%20Back%20Propagation%20Work.pdf) <br>ğŸ’» No code|
|2| Loss Functions| - MSE, MAE, Huber Loss, Binary Cross Entropy, Softmax Function, KL Divergence, Hinge Loss, Triplet Ranking Loss <br> - Pros, cons, and real-world applications of the above loss functions| âœï¸ [Written](---) <br>ğŸ’» No code|


