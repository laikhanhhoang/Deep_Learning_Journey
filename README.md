# Deep Learning Journey

## ğŸ“‘ Table of Contents

- [ğŸ“˜ About](#-about)
- [ğŸ“š Course Content](#-course-content)
- [ğŸ§  What I've Learned](#-what-ive-learned)
- [ğŸ“– Supplementary Reading](#-supplementary-reading)
- [ğŸ“ Certificate](#-certificate)

## ğŸ“˜ About

This repository documents my self-study of Deep Learning through an online Udemy course: **[The Complete Neural Networks Bootcamp: Theory, Applications](https://www.udemy.com/course/the-complete-neural-networks-bootcamp-theory-applications/)** covering core topics like MLP, CNN, RNN, Transformers, and modern applications such as NLP, CV, and LLMs.

## ğŸ§  What I've Learned

## ğŸ“– Supplementary Reading

## ğŸ“ Certificate

## ğŸ“š Course content

| Section    | Title            | Summary                                                                                                       | Note and Code                |
|------------|------------------|---------------------------------------------------------------------------------------------------------------|------------------------------|
| 1  | How Neural Networks and Backpropagation Works     | - Theory of Gradient Descent & Propagation<br>- Fwd/Bwd  calculating for a MLP | âœï¸ [W](https://github.com/laikhanhhoang/Deep_Learning_Journey/blob/main/Lecture_Note/Section%201%20-%20How%20Neural%20Networks%20and%20Back%20Propagation%20Work.pdf) <br>ğŸ’» No |
|2| Loss Functions| - MSE, MAE, Huber Loss, Binary Cross Entropy, Softmax Function, KL Divergence, Hinge Loss, Triplet Ranking Loss <br> - Pros, cons, and real-world applications of the above loss functions| âœï¸ [W](https://github.com/laikhanhhoang/Deep_Learning_Journey/blob/main/Lecture_Note/Section%202%20-%20Loss%20Functions.pdf) <br>ğŸ’» No |
|3| Activation Function| - Sigmoid, Tanh, ReLU, PreLU, ELU, GLU, Swish, Mish <br> - Properties of the above activation functions and their relation to the vanishing gradient problem| âœï¸ [W](https://github.com/laikhanhhoang/Deep_Learning_Journey/blob/main/Lecture_Note/Section%203%20-%20Activation%20Functions.pdf) <br>ğŸ’» No |
|4| Regularization and Normalization| - Regularization techniques: L1, L2, Dropout, DropConnect <br> - Normalization techniques: Min-Max, Standardization, Batch Normalization, Layer Normalization, Group Normalization and PseudoCode for those layers| âœï¸ [W](https://github.com/laikhanhhoang/Deep_Learning_Journey/blob/main/Lecture_Note/Section%204%20-%20Regularization%20and%20Normalization.pdf) <br>ğŸ’» No |
|5| Optimization| - Batch Gradient Descent, SGD, Mini-batch GD, Momentum, RMSProp, SWATS <br> - Regularization, Weight Decay, Decoupling Weight Decay, Adding Decay to SGD and ADAM| âœï¸ [W](https://github.com/laikhanhhoang/Deep_Learning_Journey/blob/main/Lecture_Note/Section%205%20-%20Optimization.pdf) <br>ğŸ’» No |
|6| Hyperparameter tuning and Learning rate Scheduling| - Step decay <br> - Cycling learning rate, Restart, Cosine annealing with Warm Restart| âœï¸ [W](https://github.com/laikhanhhoang/Deep_Learning_Journey/blob/main/Lecture_Note/Section%206%20-%20Hyperparameter%20tuning%20and%20Learning%20Rate%20Scheduling.pdf) <br>ğŸ’» No |
|7|Weight Initialization| - Understanding why initializing all weights to 0 or same value is bad <br> - Normal Distribution, Xavier Initialization, He Norm Initialization and when to use | âœï¸ [W](https://github.com/laikhanhhoang/Deep_Learning_Journey/blob/main/Lecture_Note/Section%207%20-%20Weight%20Initialization.pdf) <br>ğŸ’» No |
|8|Introduction to Pytorch| - Learning how to use Pytorch tool kits for deep learning: requires_grad, detach, no_grad, loss functions and weight initialization | âœï¸ğŸ’» [Nb](https://github.com/laikhanhhoang/Deep_Learning_Journey/blob/main/Code/Section8_Introduction_to_Pytorch.ipynb) |
|9|Data Augmentation| Center Crop, Five Crop, Random Resize, Auto Augment, Mixup, Cut mix, Random Augment,...| âœï¸ [W](https://github.com/laikhanhhoang/Deep_Learning_Journey/blob/main/Lecture_Note/Section%209%20-Data%20Augmentation.pdf) <br>ğŸ’» No |
|10| Practical NN in Pytorch with Diabetes Dataset|- Learning how to code end to end a simple Neural Network in Pytoch: how to define batch-size, optimization, loss function,...| ğŸ’» [Nb](https://github.com/laikhanhhoang/Deep_Learning_Journey/blob/main/Code/Section10_Apply_NN_in_Diabete.ipynb) |
|11| Visualize the learning process| - Practice coding a neural network with make-moon dataset as did in section 10 and plotting the learning process| ğŸ’» [Nb](https://github.com/laikhanhhoang/Deep_Learning_Journey/blob/main/Code/Section11_Visualize_Learning_Process.ipynb) |
